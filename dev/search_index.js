var documenterSearchIndex = {"docs":
[{"location":"#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"CurrentModule = LighthouseFlux","category":"page"},{"location":"","page":"API Documentation","title":"API Documentation","text":"FluxClassifier\nLighthouseFlux.loss\nLighthouseFlux.loss_and_prediction\nLighthouseFlux.evaluate_chain_in_debug_mode","category":"page"},{"location":"#LighthouseFlux.FluxClassifier","page":"API Documentation","title":"LighthouseFlux.FluxClassifier","text":"FluxClassifier(model, optimiser, classes; params=Flux.params(model),\n               onehot=(label -> Flux.onehot(label, 1:length(classes))),\n               onecold=(label -> Flux.onecold(label, 1:length(classes))))\n\nReturn a FluxClassifier <: Lighthouse.AbstractClassifier with the given arguments:\n\nmodel: a Flux model. The model must additionally support LighthouseFlux's loss and loss_and_prediction functions.\noptimiser: a Flux optimiser\nclasses: a Vector or Tuple of possible class values; this is the return\n\nvalue of Lighthouse.classes(::FluxClassifier).\n\nparams: The parameters to optimise during training; generally, a Zygote.Params\n\nvalue or a value that can be passed to Zygote.Params.\n\nonehot: the function used to convert hard labels to soft labels when\n\nLighthouse.onehot is called with this classifier.\n\nonecold: the function used to convert soft labels to hard labels when\n\nLighthouse.onecold is called with this classifier.\n\n\n\n\n\n","category":"type"},{"location":"#LighthouseFlux.loss","page":"API Documentation","title":"LighthouseFlux.loss","text":"loss(model, batch_arguments...)\n\nReturn the scalar loss of model given batch_arguments.\n\nThis method must be implemented for all models passed to FluxClassifier.\n\n\n\n\n\n","category":"function"},{"location":"#LighthouseFlux.loss_and_prediction","page":"API Documentation","title":"LighthouseFlux.loss_and_prediction","text":"loss_and_prediction(model, input_batch, other_batch_arguments...)\n\nReturn (model_loss, model_prediction) where:\n\nmodel_loss is equivalent to (and defaults to) loss(model, input_batch, other_batch_arguments...).\nmodel_prediction is a matrix where the ith column is the soft label prediction for the ith\n\nsample in input_batch. Thus, the numnber of columns should be size(input_batch)[end], while the number of rows is equal to the number of possible classes predicted by model. model_prediction defaults to model(input_batch).\n\nThis method must be implemented for all models passed to FluxClassifier, but has the default return values described above, so it only needs to be overloaded if the default definitions do not yield the expected values for a given model type. It additionally may be overloaded to avoid redundant computation if model's loss function computes soft labels as an intermediate result.\n\n\n\n\n\n","category":"function"},{"location":"#LighthouseFlux.evaluate_chain_in_debug_mode","page":"API Documentation","title":"LighthouseFlux.evaluate_chain_in_debug_mode","text":"evaluate_chain_in_debug_mode(chain::Flux.Chain, input)\n\nEvaluate chain(input), printing additional debug information at each layer.\n\n\n\n\n\n","category":"function"},{"location":"#Internal-functions","page":"API Documentation","title":"Internal functions","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"LighthouseFlux.gather_weights_gradients\nLighthouseFlux.fforeach_pairs","category":"page"},{"location":"#LighthouseFlux.gather_weights_gradients","page":"API Documentation","title":"LighthouseFlux.gather_weights_gradients","text":"gather_weights_gradients(classifier, gradients)\n\nCollects the weights and gradients from classifier into a Dict.\n\n\n\n\n\n","category":"function"},{"location":"#LighthouseFlux.fforeach_pairs","page":"API Documentation","title":"LighthouseFlux.fforeach_pairs","text":"fforeach_pairs(F, x, keys=(); exclude=Functors.isleaf, cache=IdDict(),\n               prune=Functors.NoKeyword(), combine=(ks, k) -> (ks..., k))\n\nWalks the Functors.jl-compatible graph x (by calling pairs ∘ Functors.children), applying F(parent_key, child) at each step along the way. Here parent_key is the key part of a key-value pair returned from pairs ∘ Functors.children, combined with the previous parent_key by combine.\n\nExample\n\njulia> using Functors, LighthouseFlux\n\njulia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> m = Foo(Bar([1,2,3]), (4, 5, Bar(Foo(6, 7))));\n\njulia> LighthouseFlux.fforeach_pairs((k,v) -> @show((k, v)), m)\n(k, v) = ((:x,), Bar([1, 2, 3]))\n(k, v) = ((:x, :x), [1, 2, 3])\n(k, v) = ((:y,), (4, 5, Bar(Foo(6, 7))))\n(k, v) = ((:y, 1), 4)\n(k, v) = ((:y, 2), 5)\n(k, v) = ((:y, 3), Bar(Foo(6, 7)))\n(k, v) = ((:y, 3, :x), Foo(6, 7))\n(k, v) = ((:y, 3, :x, :x), 6)\n(k, v) = ((:y, 3, :x, :y), 7)\n\nThe combine argument can be used to customize how the keys are combined. For example\n\njulia> LighthouseFlux.fforeach_pairs((k,v) -> @show((k, v)), m, \"\"; combine=(ks, k) -> string(ks, \"/\", k))\n(k, v) = (\"/x\", Bar([1, 2, 3]))\n(k, v) = (\"/x/x\", [1, 2, 3])\n(k, v) = (\"/y\", (4, 5, Bar(Foo(6, 7))))\n(k, v) = (\"/y/1\", 4)\n(k, v) = (\"/y/2\", 5)\n(k, v) = (\"/y/3\", Bar(Foo(6, 7)))\n(k, v) = (\"/y/3/x\", Foo(6, 7))\n(k, v) = (\"/y/3/x/x\", 6)\n(k, v) = (\"/y/3/x/y\", 7)\n\n\n\n\n\n\n","category":"function"}]
}
